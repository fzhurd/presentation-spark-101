{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD: Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD is the fundamental level of operation for Spark. \n",
    "* Its one of the two low level APIs in Spark, the other being Shared variables\n",
    "* You typically dont need to use this except when looking to do effecient processing that you cannot with High level APIs\n",
    "* SparkContext is how you summon the RDD functionality in Spark\n",
    "* RDDs are immutable, partitioned collection of rows that can be operated in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to know\n",
    "\n",
    "> * [Transformation](http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) : This is the manipulations you want to do on the Dataset. \n",
    "> * [Action](http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions) : You use an Action to generate the output\n",
    "> * <strong>Lazy Evaluation</strong>: Spark accumilates the transformations on a Dataset and evaluates them only when an Action is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/RDD.png)\n",
    "* Credits: image taken from [Transformation process in Apache Spark](https://stackoverflow.com/questions/39311616/transformation-process-in-apache-spark/39313146)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <strong>DAG</strong>: Directed Acyclic Graph is the execution plan that Spark generates for processing RDDs \n",
    "> * <strong>Partitions</strong>: creating subsets of Data that can be passed to the worker node for faster processing\n",
    "> * <strong>Shuffle</strong>:  shuffle is a method for re-distributing data so itâ€™s grouped differently across partitions\n",
    "> * **Spark Job**: Every Action results in a job. Every Job has **stages** and stages are a collection of **tasks**. Task is a transformation of data that will run on a single executor. Tasks are the lowest level of spark execution.\n",
    "> * **Parllelism**: combination of partitions and nodes. Parallelism defines the speed of your job. If you have one partition but many nodes => the jobs parallelism is 1. If you have many partitions but single node => the jobs parallelism is still 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Flow of a spark job\n",
    "![img](img/spark-job.png) \n",
    "* Image taken from [Understand RDD Operations: Transformations and Actions](https://trongkhoanguyen.com/spark/understand-rdd-operations-transformations-and-actions/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <strong>Broadcast</strong>: This is when you want to store some data to the worker node to make calculations faster 'data locality'\n",
    "> * <strong>Accumilators</strong>: This is used to collect the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[3]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(range(10),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {}\".format(rdd1.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd1.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd1.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI\n",
    "* localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Data','is','fun',\"Waterloo Data Science Meetup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsRDD = sc.parallelize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = wordsRDD.map(lambda wordsRDD: wordsRDD.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data'], ['is'], ['fun'], ['Waterloo', 'Data', 'Science', 'Meetup']]\n"
     ]
    }
   ],
   "source": [
    "print(flat.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.reduce(lambda w,v: w if len(w) < len(v) else v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs can also do Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 10, mean: 4.5, stdev: 2.8722813232690143, max: 9.0, min: 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.mean()\n",
    "rdd1.sum()\n",
    "rdd1.stdev()\n",
    "rdd1.variance()\n",
    "rdd1.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('AWS', 1),  ('GCP', 3), ('OpenStack', 4),('AZURE', 2), ('Oracle', 5), ('OnPrem', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[19] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AWS', 1),\n",
       " ('AZURE', 2),\n",
       " ('GCP', 3),\n",
       " ('OnPrem', 6),\n",
       " ('OpenStack', 4),\n",
       " ('Oracle', 5)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(data).sortByKey(True, 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
