{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whats all the buzz about\n",
    "\n",
    "## History\n",
    "\n",
    "* Came into existance around 2010 from University of California Berkeley Amplab. One of the makers of Apache Spark - Matei Zaharia was a University of Waterloo grad.\n",
    "* Got to prominance due to Big Data revolution. Post 2016 has gained increased adoption in enterprise data\n",
    "* Originally developed for machine learning applications but now used for data processing as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "* Spark is a open source distributed cluster computing engine used for big data processing\n",
    "* Spark provides Resilient Distributed Datasets or RDDs which can process Big Data faster than Hadoop\n",
    "* One of the reasons for the quick adoption of Spark was the difficulty of processing in Hadoop as architecturally Hadoop required reading and writing to disks during data processing that increased computational time. One of its core innovation is the ability to process data in-memory in a resilient manner on a cluster helped to reduce the time it took to process the same amount of data.\n",
    "* It is written in Scala but has API for python, Java and R\n",
    "* ![img](img/logistic-regression.png)\n",
    "Above image represents the computational speed bar plot for Hadoop and Spark for running logistic regression. Image taken from the Apache Spark website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "* Github link has the Jupyter Pysparks Docker image which can get you the Spark environment with Jupyter interface really quick\n",
    "* Bare Bones way of doing it also exist like getting the Spark link form the apache site installing the package , configuring with Jupyter but is time consuming and can run into errors while setting the environment\n",
    "* My environment is a Ubuntu Linux Virtual machine 18.04 running on Windows 10. I have the Docker installed on Linux and have pulled the latest PySpark docker image from Docker hub\n",
    "* Image comes with Batteries included, lot of the python libraries are already installed. you also have jupter interface with terminal access so you can install new libraries if its not included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture\n",
    "* This application is built for cluster computing framework. You write code and submit to the Spark Driver which parallelizes the data and executes the logic on computing nodes called Executors \n",
    "* ![img](img/spark-ach.png)\n",
    "[Credits: Image taken from Datastrophic blog](http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Driver contains the main() function and maintains information about the program input, distributes the work , schedules the execution and collects the information for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to know\n",
    "\n",
    "> * <strong>SparkSession</strong>: The driver process is instantiated using the SparkSession. Thats how Spark executes user defined logic across the cluster. Each SparkSession is a Spark Application. This creates Spark and SQL contexts\n",
    "> * <strong>Sparkcontext</strong>: This is the connection to Spark Cluster. This class help you with RDDs and shared variables\n",
    "> * <strong>Driver</strong>: This is the heart of the Spark application, it is a Java Virtual Machine that operates a Python to JAVA converter. \n",
    "> * <strong>Executor</strong>: These are executed in the compute nodes. Also a Java Virtual Machine.\n",
    "> * <strong>Mapping and Reduction</strong>: This is a programming model that is used in Big Data applications like Hadoop and Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
