{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD: Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD is the fundamental level of operation for Spark. \n",
    "* Its one of the two low level APIs in Spark, the other being Shared variables\n",
    "* You typically dont need to use this except when looking to do effecient processing that you cannot with High level APIs\n",
    "* SparkContext is how you summon the RDD functionality in Spark\n",
    "* RDDs are immutable, partitioned collection of rows that can be operated in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to know\n",
    "\n",
    "> * [Transformation](http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations): This is the manipulations you want to do on the Dataset. \n",
    "> * [Action](http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions): You use an Action to generate the output\n",
    "> * <strong>Lazy Evaluation</strong>: Spark accumilates the transformations on a Dataset and evaluates them only when an Action is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](img/RDD.png)\n",
    "* Credits: image taken from [Transformation process in Apache Spark](https://stackoverflow.com/questions/39311616/transformation-process-in-apache-spark/39313146)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <strong>DAG</strong>: Directed Acyclic Graph is the execution plan that Spark generates for processing RDDs \n",
    "> * <strong>Partitions</strong>: creating subsets of Data that can be passed to the worker node for faster processing\n",
    "> * <strong>Shuffle</strong>:  shuffle is a method for re-distributing data so itâ€™s grouped differently across partitions\n",
    "> * **Spark Job**: Every Action results in a job. Every Job has **stages** and stages are a collection of **tasks**. Task is a transformation of data that will run on a single executor. Tasks are the lowest level of spark execution.\n",
    "> * **Parllelism**: combination of partitions and nodes. Parallelism defines the speed of your job. If you have one partition but many nodes => the jobs parallelism is 1. If you have many partitions but single node => the jobs parallelism is still 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Flow of a spark job\n",
    "![img](img/spark-job.png) \n",
    "* Image taken from [Understand RDD Operations: Transformations and Actions](https://trongkhoanguyen.com/spark/understand-rdd-operations-transformations-and-actions/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <strong>Broadcast</strong>: This is when you want to store some data on the executors to make calculations faster 'data locality'. Instead of having map functions request data from Driver node and deserialise before using that in the function its simpler to have the shared variable stored on the executor so the computation is quick. Like braodcasting a lookup table to make processing faster. \n",
    "> * <strong>Accumilators</strong>: This is used to update values inside transformations and bring it to the driver. Typical use case like summing a value and propagating it to the driver for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[3]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(range(10),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of partitions: {}\".format(rdd1.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd1.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd1.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI\n",
    "* localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Data','is','fun',\"Waterloo Data Science Meetup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsRDD = sc.parallelize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = wordsRDD.map(lambda wordsRDD: wordsRDD.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flat.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsRDD.reduce(lambda w,v: w if len(w) < len(v) else v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs can also do Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.mean()\n",
    "rdd1.sum()\n",
    "rdd1.stdev()\n",
    "rdd1.variance()\n",
    "rdd1.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('AWS', 1),  ('GCP', 3), ('OpenStack', 4),('AZURE', 2), ('Oracle', 5), ('OnPrem', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize(data).sortByKey(True, 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
